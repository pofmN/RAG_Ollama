import streamlit as st
import  streamlit_toggle as tog
from streamlit_chat import message
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.llms import Ollama
from sentence_transformers import SentenceTransformer
import pdfplumber
import docx
from storage import store_document_chunks, get_relevant_chunks
import io
import hashlib
from get_context_online import get_online_context

# Page configuration
st.set_page_config(
    page_title="Document Q&A VKU Assistant",
    page_icon="ü§ñ",
    layout="wide"
)

# Initialize session state variables
if "messages" not in st.session_state:
    st.session_state.messages = []
if "language" not in st.session_state:
    st.session_state.language = "English"
if "processed_files" not in st.session_state:
    st.session_state.processed_files = set()
if "current_file_hash" not in st.session_state:
    st.session_state.current_file_hash = None

# Function to compute file hash
def compute_file_hash(file_content):
    """Compute SHA-256 hash of file content"""
    return hashlib.sha256(file_content).hexdigest()

@st.cache_resource
def load_embedding_models():
    """Load and cache embedding models"""
    return {
        "English": SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2'),
        "Vietnamese": SentenceTransformer('keepitreal/vietnamese-sbert')
    }

@st.cache_resource
def initialize_ollama():
    """Initialize and cache Ollama client"""
    return Ollama(
        base_url="http://localhost:11434",
        model="llama3.2Q5KM:latest",
    )

# Load models
embedding_models = load_embedding_models()
ollama = initialize_ollama()

def split_document(document):
    """Split document into chunks with caching"""
    try:
        file_extension = document.name.split('.')[-1].lower()
        text = ""
        
        if file_extension == 'txt':
            for encoding in ['utf-8', 'latin-1', 'ascii']:
                try:
                    text = document.getvalue().decode(encoding)
                    break
                except UnicodeDecodeError:
                    continue
        
        elif file_extension == 'pdf':
            with pdfplumber.open(io.BytesIO(document.getvalue())) as pdf:
                text = " ".join([page.extract_text() or "" for page in pdf.pages])
        
        elif file_extension == 'docx':
            doc = docx.Document(io.BytesIO(document.getvalue()))
            text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
        
        if not text:
            raise ValueError("No text could be extracted from the document")
            
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        return text_splitter.split_text(text)
    
    except Exception as e:
        st.error(f"Error processing document: {str(e)}")
        return []

# UI Components
with st.sidebar:
    st.title("Configuration")
    
    # Language selection
    new_language = st.selectbox(
        "Choose Language",
        ["English", "Vietnamese"]
    )
    
    # Handle language change
    if new_language != st.session_state.language:
        st.session_state.language = new_language
        st.session_state.embedding_model = embedding_models[new_language]
        # Clear processed files if language changes
        st.session_state.processed_files = set()
        st.rerun()
    else:
        st.session_state.embedding_model = embedding_models[st.session_state.language]
    
    st.markdown("---")
    st.subheader("Model Information")
    st.info("""
    - English: MiniLM-L6-v2
    - Vietnamese: vietnamese-sbert
    - LLM: llama3.2Q80, llama3.2Q5KM
    """)

st.title("üìö Document Q&A Assistant")

# File upload section
col1, col2 = st.columns([2, 1])
with col1:
    uploaded_file = st.file_uploader(
        "Upload your document",
        type=["txt", "pdf", "docx"],
        help="Supported formats: TXT, PDF, DOCX"
    )

# Process uploaded file
if uploaded_file is not None:
    # Compute file hash
    file_content = uploaded_file.getvalue()
    current_hash = compute_file_hash(file_content)
    
    # Check if file needs processing
    if current_hash != st.session_state.current_file_hash:
        with st.spinner("Processing new document..."):
            chunks = split_document(uploaded_file)
            if chunks:
                try:
                    store_document_chunks(chunks)
                    st.session_state.current_file_hash = current_hash   
                    st.session_state.processed_files.add(current_hash)
                    st.success(f"‚ú® Document processed and ready for questions using {st.session_state.language} models")
                except Exception as e:
                    st.error(f"Error storing embeddings: {str(e)}")
    else:
        st.success("‚ú® Document already processed and ready for questions!")

    with col2:
        st.success("‚úÖ File uploaded successfully!")
        st.info(f"üìÑ Filename: {uploaded_file.name}")
else:
    with col2:
        st.warning("‚ö†Ô∏è Please upload a document to begin")

with col2:
    st.session_state.use_internet = tog.st_toggle_switch(
        label="Use Internet for Online Search",
        key="use_internet_toggle",
        default_value=True,
        label_after=False,
        inactive_color="#D3D3D3",
        active_color="#11567f",
        track_color="#29B5E8"
    )

# Chat interface
st.markdown("---")
st.subheader("üí¨ Chat Interface")

# Display chat history
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Chat input and response generation
if prompt := st.chat_input("Ask a question about your document..."):
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Generate response
    try:
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                if st.session_state.processed_files:
                    relevant_chunks = get_relevant_chunks(prompt)
                    context = "\n".join(relevant_chunks)
                else:
                    context = get_online_context(prompt)
                
                # Prepare prompt based on language
                if st.session_state.language == "English":
                    enhanced_prompt = f"""Please use the following context: {context}
                    to answer the following question: {prompt} as detailed as possible."""
                else:
                    enhanced_prompt = f"""B·∫°n l√† m·ªôt nh√† khoa h·ªçc c√≥ th·ªÉ tr·∫£ l·ªùi m·ªçi c√¢u h·ªèi, d·ª±a tr√™n nh·ªØng th√¥ng tin sau: {context}
                    ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi {prompt} chi ti·∫øt v√† d√†i nh·∫•t c√≥ th·ªÉ. ch√∫ √Ω ch·ªâ t·∫≠p trung s·ª≠ d·ª•ng context ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi, kh√¥ng nh·∫Øc l·∫°i nh·ªØng y√™u c·∫ßu c·ªßa t√¥i ƒë·ªëi v·ªõi c√¢u h·ªèi"""

                # Generate response
                print("PROMT IS: " + enhanced_prompt)
                response = ollama.generate(
                    prompts=[enhanced_prompt],
                    generation_config={
                        'max_tokens': 8192,
                        'temperature': 0.9,
                        'top_p': 0.2,
                        'num_predict': 1024,
                        'stop': ['\n\n\n'],
                        'repeat_penalty': 1.1,
                    }
                )
                
                assistant_response = response.generations[0][0].text
                print("Answer is: "+assistant_response)
                st.markdown(assistant_response)
                
                # Add to chat history
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": assistant_response
                })
    
    except Exception as e:
        st.error(f"Error generating response: {str(e)}")

# Footer
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: #666;'>
        <small>Built with Streamlit ‚Ä¢ Powered by Nam-Giang</small>
    </div>
    """,
    unsafe_allow_html=True
)